{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jailsonpj/master-project/blob/main/exp1_cosface_loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtpE8i82wVV9"
      },
      "source": [
        "# Siamese Network - Cos Face Loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "THFmPTihR7Cr"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVljUxBLwgR-"
      },
      "source": [
        "## SetUp Libs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "KfAdM6yxwtDu"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "WveqHsPkwuqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a643299-6319-4f6c-e28a-90e8b5352632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-metric-learning in /usr/local/lib/python3.9/dist-packages (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pytorch-metric-learning) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from pytorch-metric-learning) (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from pytorch-metric-learning) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-metric-learning) (2.0.0+cu118)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (4.5.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->pytorch-metric-learning) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->pytorch-metric-learning) (3.25.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->pytorch-metric-learning) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->pytorch-metric-learning) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->pytorch-metric-learning) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.9/dist-packages (1.7.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-metric-learning\n",
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12c5k4kg2LU-"
      },
      "source": [
        "## Class Dataset\n",
        "\n",
        "- Class Article Dataset\n",
        "- Class Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "5XP89KA82DzG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.sampler import BatchSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Sh7FPghoy576"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "nICcI_rh2RNo"
      },
      "outputs": [],
      "source": [
        "# read csv datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class NewsPaperData:\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.list_filenames = [\"news-aggregator.csv\", \"news-february.csv\", \"news-july.csv\"]\n",
        "        self.df_list = list()\n",
        "\n",
        "    def read_file(self, filename):\n",
        "        return pd.read_csv(self.path + filename)\n",
        "\n",
        "    def union_title_corpus(self, df, column_title, column_corpus):\n",
        "        df[\"text\"] = df[[column_title, column_corpus]].apply(\n",
        "            lambda x: str(x[0]) + \" \" + str(x[1]), axis=1\n",
        "        )\n",
        "        return df\n",
        "    \n",
        "    def df_label_encoded(self, df, column_label):\n",
        "        df = df.query(f\"`{column_label}` == 'left' or `{column_label}` == 'right'\")\n",
        "        df[column_label] = pd.Categorical(df[column_label])\n",
        "        df[column_label] = df[column_label].cat.codes\n",
        "        return df.reset_index(drop=True)\n",
        "\n",
        "    def drop_return(self, df, index):\n",
        "        row = df.loc[index]\n",
        "        df.drop(index, inplace=True)\n",
        "        return row, df\n",
        "\n",
        "    def df_partition_label(self, df, column_label):\n",
        "        df_left = df.query(f\"`{column_label}` == 0\").reset_index(drop=True)\n",
        "        df_right = df.query(f\"`{column_label}` == 1\").reset_index(drop=True)\n",
        "\n",
        "        MAX = len(df_left) if len(df_left) < len(df_right) else len(df_right)\n",
        "        df_left, df_right = df_left[:MAX], df_right[:MAX]\n",
        "        df_all = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "        for idx in range(0, MAX):\n",
        "            row_left, df_left = self.drop_return(df_left, idx)\n",
        "            row_right, df_right = self.drop_return(df_right, idx)\n",
        "            df_all = df_all.append(row_left.to_dict(), ignore_index=True)\n",
        "            df_all = df_all.append(row_right.to_dict(), ignore_index=True)\n",
        "\n",
        "        return df_all\n",
        "\n",
        "    def rename_columns(self, df, dict_map):\n",
        "        return df.rename(columns=dict_map)\n",
        "\n",
        "    def get_dataset_custom(self):\n",
        "        dict_map = {\"Bias\": \"labels\"}\n",
        "\n",
        "        for filename in self.list_filenames:\n",
        "            self.df_list.append(\n",
        "                self.read_file(filename)\n",
        "            )\n",
        "\n",
        "        df = pd.concat(self.df_list, ignore_index=True)\n",
        "        df = self.union_title_corpus(df, \"Title\", \"Content\")\n",
        "        df = self.df_label_encoded(df, \"Bias\")\n",
        "        df = self.rename_columns(df, dict_map)\n",
        "\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "TTEJmjBkyJsK"
      },
      "outputs": [],
      "source": [
        "dataset = NewsPaperData(\"/content/drive/MyDrive/mestrado/src/\")\n",
        "df = dataset.get_dataset_custom()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "HlIuhVlAy8jy",
        "outputId": "6b2af3f6-c228-4cee-a170-63e64bc5d9fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  Source                                              Title  \\\n",
              "0                cnn.com                               Stocks slip slightly   \n",
              "1          investors.com    Stocks Down At Midday On Weak China Export Data   \n",
              "2                cnn.com              Winners and losers of the bull market   \n",
              "3            foxnews.com  Does Mt. Gox CEO still control all those stole...   \n",
              "4           mashable.com  Bankrupt Exchange Mt. Gox Still Has $600 Milli...   \n",
              "..                   ...                                                ...   \n",
              "995  whitehousewatch.com  Dan Froomkin's White House Watch  Disaster and...   \n",
              "996  whitehousewatch.com  Give up on getting this White House to answer ...   \n",
              "997  whitehousewatch.com  The death of Jamal Khashoggi reveals a univers...   \n",
              "998  whitehousewatch.com  Can a president keep official actions secret e...   \n",
              "999  whitehousewatch.com  President says crazy thing were not going to d...   \n",
              "\n",
              "                                               Content  labels  \\\n",
              "0    With little U.S. economic or corporate news on...       0   \n",
              "1    Stocks were down across the board at midday Mo...       1   \n",
              "2    It's been five years since the stock market hi...       0   \n",
              "3    next Image 1 of 3 prev next Image 2 of 3 prev ...       1   \n",
              "4    Japan-based Bitcoin exchange Mt.Gox has been d...       0   \n",
              "..                                                 ...     ...   \n",
              "995  Over the past several years, a considerable nu...       0   \n",
              "996  Its time to declare press relations with this ...       0   \n",
              "997  Reports from Turkish sources suggest that diss...       0   \n",
              "998  Donald Trumps unprecedented and deeply suspici...       0   \n",
              "999  It happened again today. The president of the ...       0   \n",
              "\n",
              "                                                  text  \n",
              "0    Stocks slip slightly With little U.S. economic...  \n",
              "1    Stocks Down At Midday On Weak China Export Dat...  \n",
              "2    Winners and losers of the bull market It's bee...  \n",
              "3    Does Mt. Gox CEO still control all those stole...  \n",
              "4    Bankrupt Exchange Mt. Gox Still Has $600 Milli...  \n",
              "..                                                 ...  \n",
              "995  Dan Froomkin's White House Watch  Disaster and...  \n",
              "996  Give up on getting this White House to answer ...  \n",
              "997  The death of Jamal Khashoggi reveals a univers...  \n",
              "998  Can a president keep official actions secret e...  \n",
              "999  President says crazy thing were not going to d...  \n",
              "\n",
              "[1000 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b5519d58-52ae-4b3a-bccd-70cf5f508a11\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Title</th>\n",
              "      <th>Content</th>\n",
              "      <th>labels</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cnn.com</td>\n",
              "      <td>Stocks slip slightly</td>\n",
              "      <td>With little U.S. economic or corporate news on...</td>\n",
              "      <td>0</td>\n",
              "      <td>Stocks slip slightly With little U.S. economic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>investors.com</td>\n",
              "      <td>Stocks Down At Midday On Weak China Export Data</td>\n",
              "      <td>Stocks were down across the board at midday Mo...</td>\n",
              "      <td>1</td>\n",
              "      <td>Stocks Down At Midday On Weak China Export Dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cnn.com</td>\n",
              "      <td>Winners and losers of the bull market</td>\n",
              "      <td>It's been five years since the stock market hi...</td>\n",
              "      <td>0</td>\n",
              "      <td>Winners and losers of the bull market It's bee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>foxnews.com</td>\n",
              "      <td>Does Mt. Gox CEO still control all those stole...</td>\n",
              "      <td>next Image 1 of 3 prev next Image 2 of 3 prev ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Does Mt. Gox CEO still control all those stole...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mashable.com</td>\n",
              "      <td>Bankrupt Exchange Mt. Gox Still Has $600 Milli...</td>\n",
              "      <td>Japan-based Bitcoin exchange Mt.Gox has been d...</td>\n",
              "      <td>0</td>\n",
              "      <td>Bankrupt Exchange Mt. Gox Still Has $600 Milli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>whitehousewatch.com</td>\n",
              "      <td>Dan Froomkin's White House Watch  Disaster and...</td>\n",
              "      <td>Over the past several years, a considerable nu...</td>\n",
              "      <td>0</td>\n",
              "      <td>Dan Froomkin's White House Watch  Disaster and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>whitehousewatch.com</td>\n",
              "      <td>Give up on getting this White House to answer ...</td>\n",
              "      <td>Its time to declare press relations with this ...</td>\n",
              "      <td>0</td>\n",
              "      <td>Give up on getting this White House to answer ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>whitehousewatch.com</td>\n",
              "      <td>The death of Jamal Khashoggi reveals a univers...</td>\n",
              "      <td>Reports from Turkish sources suggest that diss...</td>\n",
              "      <td>0</td>\n",
              "      <td>The death of Jamal Khashoggi reveals a univers...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>whitehousewatch.com</td>\n",
              "      <td>Can a president keep official actions secret e...</td>\n",
              "      <td>Donald Trumps unprecedented and deeply suspici...</td>\n",
              "      <td>0</td>\n",
              "      <td>Can a president keep official actions secret e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>whitehousewatch.com</td>\n",
              "      <td>President says crazy thing were not going to d...</td>\n",
              "      <td>It happened again today. The president of the ...</td>\n",
              "      <td>0</td>\n",
              "      <td>President says crazy thing were not going to d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b5519d58-52ae-4b3a-bccd-70cf5f508a11')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b5519d58-52ae-4b3a-bccd-70cf5f508a11 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b5519d58-52ae-4b3a-bccd-70cf5f508a11');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "df[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "0KCylS7-1Vwn"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.article_text = dataframe.text\n",
        "        self.targets = self.data.labels\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.article_text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        article_text = str(self.article_text[index])\n",
        "        article_text = \" \".join(article_text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            article_text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "DWJPrtmn2C59"
      },
      "outputs": [],
      "source": [
        "df_train = df[:500].reset_index()\n",
        "df_val = df[500:1000].reset_index()\n",
        "df_test = df[1000:1500].reset_index()\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "training_set = CustomDataset(df_train, tokenizer, 512)\n",
        "validating_set = CustomDataset(df_val, tokenizer, 512)\n",
        "testing_set = CustomDataset(df_test, tokenizer, 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "aKRlodxH2ogU"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 4\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "validating_loader = DataLoader(validating_set, **test_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqS6MiX-22nO"
      },
      "source": [
        "## SetUP Loss\n",
        "- Loss utilized is `Cos Face Recognition`, combined with optimizer `Adam`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "vIBUggmK2oJ0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pytorch_metric_learning import distances, losses, miners, reducers, testers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "VxXyJzT6284K"
      },
      "outputs": [],
      "source": [
        "### pytorch-metric-learning stuff ###\n",
        "num_classes = 2\n",
        "embedding_size = 256\n",
        "loss_func = losses.CosFaceLoss(num_classes, embedding_size).to(device=\"cuda\")\n",
        "loss_optimizer = torch.optim.Adam(loss_func.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRTP7XE_3BYX"
      },
      "source": [
        "## Class Network\n",
        "- Class BERTClass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "8QRgvOuu3EEU"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "\n",
        "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
        "\n",
        "class BERTClass(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BERTClass, self).__init__()\n",
        "    self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
        "    self.l3 = torch.nn.Linear(768, 256)\n",
        "  \n",
        "  def forward(self, ids, mask, token_type_ids):\n",
        "    _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "    output = self.l3(output_1)\n",
        "    return output\n",
        "    \n",
        "  def get_embedding(self, ids, mask, token_type_ids):\n",
        "    return self.forward(ids, mask, token_type_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEjZhDP53R1W",
        "outputId": "f8f9fc34-863a-4b97-baa1-aa796f6df31a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTClass(\n",
              "  (l1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (l3): Linear(in_features=768, out_features=256, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "bert_model = BERTClass()\n",
        "bert_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kncV2j493UjP"
      },
      "source": [
        "## Train Model\n",
        "- Train: Function training model\n",
        "\n",
        "- Test: Funtion Validating model\n",
        "\n",
        "- fit:  Function execution process training and validating model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss(num_epochs, train_loss, val_loss):\n",
        "  plt.plot(range(0, num_epochs), train_loss, \"-b\", label=\"train-loss\")\n",
        "  plt.plot(range(0, num_epochs), val_loss, \"-r\", label=\"val-loss\")\n",
        "  plt.xlabel(\"Epochs Iteration\")\n",
        "  plt.legend(loc=\"upper left\")\n",
        "  plt.title(\"Loss Train and Validate\")\n",
        "  plt.savefig(\"Loss_Validate.png\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "gjhJIDgkIHGs"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "AitWuKpK4Uyw"
      },
      "outputs": [],
      "source": [
        "def train(model, loss_func, loss_optimizer, device, train_loader, optimizer, mining_func=None):\n",
        "  model.train()\n",
        "  losses = []\n",
        "  total_loss = 0\n",
        "\n",
        "  for batch_idx, data in enumerate(train_loader):\n",
        "    ids = data['ids'].to(device, dtype=torch.int64)\n",
        "    mask = data['mask'].to(device, dtype=torch.int64)\n",
        "    token_type_ids = data['token_type_ids'].to(device, dtype=torch.int64)\n",
        "    targets = data['targets'].to(device, torch.float32)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss_optimizer.zero_grad()\n",
        "\n",
        "    embeddings = model(ids, mask, token_type_ids)\n",
        "\n",
        "    loss = loss_func(embeddings, targets)\n",
        "    losses.append(loss.item())\n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    loss_optimizer.step()\n",
        "\n",
        "    if batch_idx % 20 == 0:\n",
        "      message = \"Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "          batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), np.mean(losses)\n",
        "      )\n",
        "      print(message)\n",
        "      losses = []\n",
        "\n",
        "    total_loss /= (batch_idx + 1)\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "TLdzsm3T9wB_"
      },
      "outputs": [],
      "source": [
        "def test(model, loss_func, device, val_loader, mining_func=None):\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    for batch_idx, data in enumerate(val_loader):\n",
        "      ids = data['ids'].to(device, dtype=torch.int64)\n",
        "      mask = data['mask'].to(device, dtype=torch.int64)\n",
        "      token_type_ids = data['token_type_ids'].to(device, dtype=torch.int64)\n",
        "      targets = data['targets'].to(device, torch.float32)\n",
        "\n",
        "      embeddings = model(ids, mask, token_type_ids)\n",
        "\n",
        "      loss_outputs = loss_func(embeddings, targets)\n",
        "\n",
        "      loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n",
        "      val_loss += loss.item()\n",
        "\n",
        "    return val_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(train_loader, val_loader, model, loss_func, loss_optimizer, optimizer, scheduler, num_epochs, device, loss_scheduler, mining_func=None, start_epoch=0):\n",
        "  for epoch in range(0, start_epoch):\n",
        "    scheduler.step()\n",
        "    loss_scheduler.step()\n",
        "\n",
        "  train_plot, val_plot = [], []\n",
        "  for epoch in range(start_epoch, num_epochs):\n",
        "    scheduler.step()\n",
        "\n",
        "    # Train stage\n",
        "    train_loss = train(model, loss_func, loss_optimizer, device, train_loader, optimizer)\n",
        "    train_plot.append(train_loss)\n",
        "\n",
        "    message = \"Epoch: {}/{}. Train set: Average Loss: {:.4f}\".format(epoch + 1, num_epochs, train_loss)\n",
        "\n",
        "    # Validation Stage\n",
        "    val_loss = test(model, loss_func, device, val_loader)\n",
        "    val_loss /= len(val_loader)\n",
        "    val_plot.append(val_loss)\n",
        "    message += \"\\nEpoch: {}/{}. Validation set: Average Loss: {:.4f}\".format(epoch + 1, num_epochs,\n",
        "                                                                             val_loss)\n",
        "    print(message)\n",
        "    \n",
        "  plot_loss(num_epochs, train_plot, val_plot)"
      ],
      "metadata": {
        "id": "qRxk1DuHDfur"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "UnD_fjGV90j6"
      },
      "outputs": [],
      "source": [
        "from torch.optim import lr_scheduler\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(bert_model.parameters(), lr=1e-4)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "loss_scheduler = optim.lr_scheduler.StepLR(loss_optimizer, step_size=1, gamma=0.1)\n",
        "num_epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "id": "OjZTpuNd962I",
        "outputId": "563d8929-a3fd-4ea4-b71e-ea24ceb62dac"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-06913c161468>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidating_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_scheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-85-5e8d370c41b6>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(train_loader, val_loader, model, loss_func, loss_optimizer, optimizer, scheduler, num_epochs, device, loss_scheduler, mining_func, start_epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Train stage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_plot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-c3891a142fdd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss_func, loss_optimizer, device, train_loader, optimizer, mining_func)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_metric_learning/losses/base_metric_loss_function.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embeddings, labels, indices_tuple, ref_emb, ref_labels)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mref_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ref_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         loss_dict = self.compute_loss(\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_metric_learning/losses/large_margin_softmax_loss.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, embeddings, labels, indices_tuple, ref_emb, ref_labels)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mminer_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_target_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mcosine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mcosine_of_target_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_metric_learning/losses/large_margin_softmax_loss.py\u001b[0m in \u001b[0;36mget_target_mask\u001b[0;34m(self, embeddings, labels)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tensors used as indices must be long, int, byte or bool tensors"
          ]
        }
      ],
      "source": [
        "fit(training_loader, validating_loader, bert_model, loss_func, loss_optimizer, optimizer, scheduler, num_epochs, device, loss_scheduler)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation Model"
      ],
      "metadata": {
        "id": "u9BEEvTVGI-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_embeddings(dataloader, model):\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    embeddings = np.zeros((len(dataloader.dataset), 256))\n",
        "    labels = np.zeros(len(dataloader.dataset))\n",
        "    k = 0\n",
        "    for batch_idx, data in enumerate(dataloader):\n",
        "      ids = data['ids'].to(device, dtype = torch.long)\n",
        "      mask = data['mask'].to(device, dtype = torch.long)\n",
        "      token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "      targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "      embeddings[k:k+len(data)] = model.get_embedding(ids, mask, token_type_ids).data.cpu().numpy()\n",
        "      labels[k:k+len(data)] = targets.data.cpu().numpy()\n",
        "      k += len(data)\n",
        "\n",
        "  return embeddings, labels\n"
      ],
      "metadata": {
        "id": "FXg_7J4QGP7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_embeddings_cl, testing_labels_cl = extract_embeddings(testing_loader, model)"
      ],
      "metadata": {
        "id": "Uf-5_jsmH_fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save test embeddings for visualization in projector\n",
        "import io\n",
        "np.savetxt(\"vecs.tsv\", testing_embeddings_cl, delimiter='\\t')\n",
        "\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "[out_m.write(str(x) + \"\\n\") for x in testing_labels_cl]\n",
        "out_m.close()"
      ],
      "metadata": {
        "id": "2BAi9xkaPXOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model CLassification\n",
        "- Logistic Regression\n",
        "- Decision Tree"
      ],
      "metadata": {
        "id": "Jc066pCVPpnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X, y):\n",
        "  clf = model.fit(X, y)\n",
        "  return clf\n",
        "\n",
        "def predict_model(clf, X):\n",
        "  return clf.predict(X)"
      ],
      "metadata": {
        "id": "ANc__5wMRA45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "clf = LogisticRegression(random_state=0)\n",
        "clf_trained = train_model(clf, testing_embeddings_cl[:500], testing_labels_cl[:500])\n",
        "predicts_result = predict_model(clf_trained, testing_embeddings_cl[500:])\n",
        "\n",
        "print(\"Accuracy Logistic Regression: \", accuracy_score(testing_labels_cl[500:], predicts_result))\n",
        "print(\"-------------------------------\")\n",
        "print(\"F1 Score Logistic Regression: \", f1_score(testing_labels_cl[500:], predicts_result))\n",
        "print(\"-------------------------------\")\n",
        "print(\"Confusion Matrix Logistic Regression\\n: \", confusion_matrix(testing_labels_cl[500:], predicts_result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4osLo5LRk8A",
        "outputId": "247b1203-5a14-4eee-dbec-b52802bbf3c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Logistic Regression:  0.628\n",
            "-------------------------------\n",
            "F1 Score Logistic Regression:  0.7714987714987714\n",
            "-------------------------------\n",
            "Confusion Matrix Logistic Regression\n",
            ":  [[  0 186]\n",
            " [  0 314]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "result_cross = cross_val_score(clf, testing_embeddings_cl[:500], testing_labels_cl[:500], cv=10)\n",
        "\n",
        "clf_trained = train_model(clf, testing_embeddings_cl[:500], testing_labels_cl[:500])\n",
        "predicts_result = predict_model(clf_trained, testing_embeddings_cl[500:])\n",
        "\n",
        "print(\"Cross Validation: \", result_cross)\n",
        "print(\"Accuracy Decision Tree: \", accuracy_score(testing_labels_cl[500:], predicts_result))\n",
        "print(\"-------------------------------\")\n",
        "print(\"F1 Score Decision Tree: \", f1_score(testing_labels_cl[500:], predicts_result))\n",
        "print(\"-------------------------------\")\n",
        "print(\"Confusion Matrix Decision Tree\\n: \", confusion_matrix(testing_labels_cl[500:], predicts_result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUtNx7j4ZBZ7",
        "outputId": "dbf13f30-5a94-4d47-f6d8-ba273203f882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Validation:  [0.54 0.4  0.52 0.6  0.64 0.48 0.48 0.56 0.6  0.6 ]\n",
            "Accuracy Logistic Regression:  0.568\n",
            "-------------------------------\n",
            "F1 Score Logistic Regression:  0.6737160120845921\n",
            "-------------------------------\n",
            "Confusion Matrix Logistic Regression\n",
            ":  [[ 61 125]\n",
            " [ 91 223]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FptXc3pgaMpl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KVljUxBLwgR-",
        "12c5k4kg2LU-",
        "TRTP7XE_3BYX"
      ],
      "machine_shape": "hm",
      "mount_file_id": "1uKCO7AiU4DlGZZKXazVprL5SuSU9Iybq",
      "authorship_tag": "ABX9TyNWYDkMRVgMXR4L5fJWenms",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}